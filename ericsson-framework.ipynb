{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c182a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mibam\\Documents\\GitHub\\code-comprehension\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import getpass\n",
    "import pandas as pd\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from typing import Union, List\n",
    "\n",
    "# Validation\n",
    "sys.path.append(os.path.abspath('baseline.py'))\n",
    "from baseline import evaluate_docstring\n",
    "\n",
    "# Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "sys.path.append(os.path.abspath('package'))\n",
    "\n",
    "#from package.hierarchical_graph import HierarchicalGraphBuilder, FunctionGraphBuilder\n",
    "\n",
    "# Python imports\n",
    "from typing import Union, Dict, Optional, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Disable all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bd9e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token: # Token: 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\n",
    "# BALUE Token: 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\n",
    "HUGGINGFACE_API_KEY = getpass.getpass(\"Enter your Hugging Face API key: \")\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe8fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Prompt Template\n",
    "# -----------------------------\n",
    "PROMPT_TEMPLATE =\"\"\"\n",
    "    You are a helpful assistant that writes Python docstrings following best practices.\n",
    "\n",
    "    Given the context below, generate a well-formatted docstring using the following structure:\n",
    "    - A one-line summary of what the function does.\n",
    "    - A description of each parameter (if any).\n",
    "    - A description of the return value (if any).\n",
    "\n",
    "    Use the format:\n",
    "    \\\"\\\"\\\"<summary>\n",
    "\n",
    "    Args:\n",
    "        param1 (type): Description.\n",
    "        param2 (type): Description.\n",
    "\n",
    "    Returns:\n",
    "        type: Description.\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    Context:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "763a442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeAttentionEmbedder:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", device: str = None):\n",
    "        \"\"\"\n",
    "        A class for embedding textual data (e.g., docstrings) using SentenceTransformer models.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the pre-trained sentence transformer model.\n",
    "            device (str): Device to use (\"cuda\", \"cpu\", or None for auto-detect).\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    def encode(self, texts: Union[str, List[str]], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Encode one or more text strings into sentence embeddings.\n",
    "\n",
    "        Args:\n",
    "            texts (str or List[str]): Text(s) to encode.\n",
    "            batch_size (int): Batch size for encoding.\n",
    "\n",
    "        Returns:\n",
    "            List[List[float]]: A list of embedding vectors.\n",
    "        \"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        embeddings = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            convert_to_tensor=False,\n",
    "            show_progress_bar=len(texts) > batch_size\n",
    "        )\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c49e0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocstringPredictorGNN(nn.Module):\n",
    "    def __init__(self, out_channels: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "\n",
    "        self.convs.append(HeteroConv({\n",
    "            ('function', 'calls', 'function'): SAGEConv((384, 384), 256)\n",
    "        }, aggr='sum'))\n",
    "        \n",
    "        self.convs.append(HeteroConv({\n",
    "            ('expression', 'connected_to', 'expression'): SAGEConv((768, 768), 512)\n",
    "        }, aggr='sum'))\n",
    "\n",
    "        self.convs.append(HeteroConv({\n",
    "            ('expression', 'connected_to', 'expression'): SAGEConv((512, 512), 256)\n",
    "        }, aggr='sum'))\n",
    "\n",
    "        self.convs.append(HeteroConv({\n",
    "            ('function', 'contains', 'expression'): SAGEConv((256, 256), out_channels),\n",
    "            ('expression', 'is_in', 'function'): SAGEConv((256, 256), out_channels)\n",
    "        }, aggr='sum'))\n",
    "\n",
    "        self.convs.append(HeteroConv({\n",
    "            ('function', 'calls', 'function'): SAGEConv((out_channels, out_channels), out_channels)\n",
    "        }, aggr='sum'))\n",
    "\n",
    "        self.convs.append(HeteroConv({\n",
    "            ('expression', 'connected_to', 'expression'): SAGEConv((out_channels, out_channels), out_channels)\n",
    "        }, aggr='sum'))\n",
    "\n",
    "        self.convs.append(HeteroConv({\n",
    "            ('expression', 'connected_to', 'expression'): SAGEConv((out_channels, out_channels), out_channels)\n",
    "        }, aggr='sum'))\n",
    "\n",
    "        self.convs.append(HeteroConv({\n",
    "            ('expression', 'is_in', 'function'): SAGEConv((out_channels, out_channels), out_channels)\n",
    "        }, aggr='sum'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, data: HeteroData):\n",
    "        x_dict, edge_index_dict = data.x_dict, data.edge_index_dict\n",
    "\n",
    "        for _, conv in enumerate(self.convs):\n",
    "            if conv.convs.keys() == [('function', 'calls', 'function')]:\n",
    "                x_dict['function'] = F.leaky_relu(conv(x_dict, edge_index_dict)['function'])\n",
    "            elif conv.convs.keys() == [('expression', 'connected_to', 'expression')]:\n",
    "                x_dict['expression'] = F.leaky_relu(conv(x_dict, edge_index_dict)['expression'])\n",
    "            elif conv.convs.keys() == [('function', 'contains', 'expression'), ('expression', 'is_in', 'function')]:\n",
    "                x_dict = conv(x_dict, edge_index_dict)\n",
    "                x_dict['function'] = F.leaky_relu(x_dict['function'])\n",
    "                x_dict['expression'] = F.leaky_relu(x_dict['expression'])\n",
    "            elif conv.convs.keys() == [('expression', 'isin', 'function')]:\n",
    "                x_dict['function'] = F.leaky_relu(conv(x_dict, edge_index_dict)['function'])\n",
    "\n",
    "        return x_dict['function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21297efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Multimodal Fusion + LLaMA Inference\n",
    "# -----------------------------\n",
    "class MultimodalCommentPipeline(nn.Module):\n",
    "    def __init__(self, model:str=\"meta-llama/Meta-Llama-3-8B\", text_embedder_model:str=\"all-MiniLM-L6-v2\", gcn_out_channels:int=128):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.code_embedder = CodeAttentionEmbedder(model_name=text_embedder_model, device=self.device)\n",
    "        self.gnn = DocstringPredictorGNN(out_channels=gcn_out_channels).to(self.device)\n",
    "\n",
    "        self.fusion = nn.Linear(384 + gcn_out_channels, 1024).to(self.device)\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            model, \n",
    "            torch_dtype=torch.float16,\n",
    "            token=HUGGINGFACE_API_KEY\n",
    "        ).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Ensure special token is in tokenizer\n",
    "        self.special_token = \"<context>\"\n",
    "        if self.special_token not in self.tokenizer.get_vocab():\n",
    "            self.tokenizer.add_tokens([self.special_token])\n",
    "            self.llm.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        function_bodys: List[str],\n",
    "        code_graph: HeteroData,\n",
    "        code_graph_batch_mask: List[int],\n",
    "        docstrings: Optional[List[str]] = None,\n",
    "        mode: str = \"train\",\n",
    "        alpha: float = 0.5,\n",
    "        max_new_tokens: int = 250\n",
    "    ) -> Union[Dict[str, torch.Tensor], Dict[str, List[str]]]:\n",
    "\n",
    "        batch_size = len(function_bodys)\n",
    "        device = self.device\n",
    "        results = []\n",
    "        total_ce_loss = 0.0\n",
    "        total_cos_loss = 0.0\n",
    "        total_comb_loss = 0.0\n",
    "\n",
    "        # Step 1: Embed all function bodies at once (batched)\n",
    "        with torch.no_grad():\n",
    "            code_embs = torch.tensor(\n",
    "                self.code_embedder.encode(function_bodys),\n",
    "                dtype=torch.float32,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "        # Step 2: Run GNN (already batched via code_graph_batch_mask)\n",
    "        graph_embs = self.gnn(code_graph)[code_graph_batch_mask].to(device)\n",
    "\n",
    "        # Step 3: Fuse embeddings\n",
    "        fused_embs = self.fusion(torch.cat([code_embs, graph_embs], dim=-1))  # (B, 1024)\n",
    "\n",
    "        # Step 4: Tokenize prompts in batch\n",
    "        prompts = [PROMPT_TEMPLATE + f\" {self.special_token}\" for _ in range(batch_size)]\n",
    "        tokenized = self.tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "        prompt_embeds = self.llm.get_input_embeddings()(input_ids)\n",
    "\n",
    "        # Step 5: Locate special token positions\n",
    "        placeholder_id = self.tokenizer.convert_tokens_to_ids(self.special_token)\n",
    "        placeholder_mask = (input_ids == placeholder_id)  # (B, L)\n",
    "\n",
    "        # Sanity check: Ensure all prompts have the special token\n",
    "        assert placeholder_mask.any(dim=1).all(), \"Special token missing in one or more prompts.\"\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            placeholder_index = placeholder_mask[i].nonzero(as_tuple=True)[0].item()\n",
    "            prompt_embeds[i, placeholder_index] = fused_embs[i]\n",
    "\n",
    "        if mode == \"train\" and docstrings is not None:\n",
    "            # Tokenize all gold docstrings\n",
    "            max_total_len = 512\n",
    "            gold_tokenized = self.tokenizer(\n",
    "                docstrings,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_total_len - prompt_embeds.shape[1]\n",
    "            ).to(device)\n",
    "\n",
    "            gold_ids = gold_tokenized.input_ids\n",
    "            gold_embeds = self.llm.get_input_embeddings()(gold_ids)\n",
    "\n",
    "            input_embeds = torch.cat([prompt_embeds, gold_embeds], dim=1)\n",
    "\n",
    "            pad_labels = torch.full((batch_size, prompt_embeds.shape[1]), -100).to(device)\n",
    "            full_labels = torch.cat([pad_labels, gold_ids], dim=1)\n",
    "\n",
    "            outputs = self.llm(\n",
    "                inputs_embeds=input_embeds,\n",
    "                labels=full_labels,\n",
    "                return_dict=True,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "\n",
    "            ce_loss = outputs.loss\n",
    "\n",
    "            with torch.no_grad():\n",
    "                ref_embed = gold_embeds.mean(dim=1)\n",
    "            gen_embed = outputs.hidden_states[-1].mean(dim=1)\n",
    "            cos_loss = 1 - F.cosine_similarity(gen_embed, ref_embed, dim=1).mean()\n",
    "\n",
    "            comb_loss = ce_loss + alpha * cos_loss\n",
    "\n",
    "            return {\n",
    "                \"loss\": comb_loss,\n",
    "                \"ce_loss\": ce_loss,\n",
    "                \"cos_loss\": cos_loss\n",
    "            }\n",
    "\n",
    "        elif mode == \"generate\":\n",
    "            attention = torch.ones(prompt_embeds.shape[:-1]).to(device)\n",
    "\n",
    "            output_ids = self.llm.generate(\n",
    "                inputs_embeds=prompt_embeds,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                attention_mask=attention\n",
    "            )\n",
    "\n",
    "            decoded = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            return {\"generated\": decoded}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_parameters(model: MultimodalCommentPipeline):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'gnn' in name or 'fusion' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "def train(model: MultimodalCommentPipeline, dataset: pd.DataFrame, graph: HeteroData, num_epochs: int = 10, lr: float = 5e-5, batch_size: int = 1):\n",
    "\n",
    "    freeze_parameters(model)\n",
    "    optimizer = AdamW(filter(lambda p: p.requires_grad,model.parameters()), lr=lr)\n",
    "    tokenizer = model.tokenizer\n",
    "\n",
    "    graph = graph.to(model.device)\n",
    "\n",
    "\n",
    "    # Feltételezve, hogy a graph egy HeteroData objektum\n",
    "    function_x = graph['function'].x\n",
    "\n",
    "    # 1. Kiszűrjük a nem nullvektoros function node-okat\n",
    "    valid_CG_nodes_mask = function_x.abs().sum(dim=1) != 0\n",
    "    valid_node_indices = valid_CG_nodes_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # 2. Válasszuk ki ezek 35%-át maszkolásra\n",
    "    num_to_mask = int(0.35 * valid_node_indices.size(0))\n",
    "    perm = torch.randperm(valid_node_indices.size(0))\n",
    "    masked_nodes = valid_node_indices[perm[:num_to_mask]]\n",
    "    non_masked_nodes = valid_node_indices[perm[num_to_mask:]]\n",
    "\n",
    "    # 3. A maszkolt node-ok 60%-a megy train maskba, többi validációba\n",
    "    num_masked_train = int(0.6 * masked_nodes.size(0))\n",
    "    masked_train_nodes = masked_nodes[:num_masked_train]\n",
    "    masked_val_nodes = masked_nodes[num_masked_train:]\n",
    "\n",
    "    # 4. Train mask: nem maszkolt + maszkolt train node-ok\n",
    "    train_nodes = torch.cat([non_masked_nodes, masked_train_nodes])\n",
    "\n",
    "    # 5. Nullázzuk a maszkolt node-ok embeddingjeit\n",
    "    function_x[masked_nodes] = 0.0\n",
    "\n",
    "    # 6. Train és validation maszkok létrehozása\n",
    "    train_mask = torch.zeros(function_x.size(0), dtype=torch.bool)\n",
    "    val_mask = torch.zeros(function_x.size(0), dtype=torch.bool)\n",
    "    train_mask[train_nodes] = True\n",
    "    val_mask[masked_val_nodes] = True\n",
    "\n",
    "    train_dataset = dataset[train_mask.tolist()].copy()\n",
    "    train_dataset['original_index'] = train_dataset.index\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    val_dataset = dataset[val_mask.tolist()].copy()\n",
    "    val_dataset['original_index'] = val_dataset.index\n",
    "    val_dataset = val_dataset.reset_index(drop=True)\n",
    "\n",
    "    # Mask valid CG node docstrings\n",
    "    generated_docstrings = []\n",
    "    comb_losses = []\n",
    "    ce_losses = []\n",
    "    cos_losses = []\n",
    "    val_comb_losses = []\n",
    "    val_ce_losses = []\n",
    "    val_cos_losses = []\n",
    "    for epoch in tqdm(range(num_epochs),desc=\"Training...\"):\n",
    "        model.train()\n",
    "        total_comb_loss = 0.0\n",
    "        total_ce_loss = 0.0\n",
    "        total_cos_loss = 0.0\n",
    "        valid_samples = 0\n",
    "\n",
    "        for idx in range(0, len(train_dataset), batch_size):\n",
    "\n",
    "            batch_df = train_dataset.iloc[idx:idx+batch_size]\n",
    "    \n",
    "            # Filter invalid samples\n",
    "            valid_mask = (\n",
    "                batch_df[\"docstring\"].notna() &\n",
    "                (batch_df[\"docstring\"] != \"\") &\n",
    "                (batch_df[\"docstring\"].astype(str).str.lower() != \"nan\")\n",
    "            )\n",
    "            batch_df = batch_df[valid_mask]\n",
    "            \n",
    "            if batch_df.empty:\n",
    "                continue\n",
    "\n",
    "            function_bodies = batch_df[\"function_code\"].tolist()\n",
    "            function_docstrings = batch_df[\"docstring\"].tolist()\n",
    "            graph_batch_mask_prep = batch_df['original_index'].tolist()\n",
    "\n",
    "            # Construct graph_batch_mask\n",
    "            graph_batch_mask = torch.zeros(function_x.size(0), dtype=torch.bool).to(model.device)\n",
    "            graph_batch_mask[graph_batch_mask_prep] = True\n",
    "\n",
    "            # Forward pass – generate output\n",
    "            output = model(\n",
    "                function_bodys=function_bodies,\n",
    "                code_graph=graph,\n",
    "                code_graph_batch_mask=graph_batch_mask,\n",
    "                docstrings=function_docstrings,\n",
    "                mode=\"train\"\n",
    "            )\n",
    "            #print(output)\n",
    "            loss = output[\"loss\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print(\"Backward prop completed. Optimizer step done.\")\n",
    "\n",
    "            total_comb_loss += loss.item()\n",
    "            total_ce_loss += output[\"ce_loss\"].item()\n",
    "            total_cos_loss += output[\"cos_loss\"].item()\n",
    "            valid_samples += 1\n",
    "\n",
    "        # Avoid division by zero if all samples were skipped\n",
    "        if valid_samples == 0:\n",
    "            print(f\"Epoch {epoch+1}: No valid samples to train on.\")\n",
    "            continue\n",
    "        \n",
    "        # Validation phase\n",
    "        #print(\"Starting validation\")\n",
    "        model.eval()\n",
    "        total_val_comb_loss = 0.0\n",
    "        total_val_ce_loss = 0.0\n",
    "        total_val_cos_loss = 0.0\n",
    "        valid_val_samples = 0\n",
    "        \n",
    "        generated_docstrings = []\n",
    "        with torch.no_grad():\n",
    "            for idx in range(0, len(val_dataset), batch_size):\n",
    "                batch_df = val_dataset.iloc[idx:idx+batch_size]\n",
    "    \n",
    "                # Filter invalid samples\n",
    "                valid_mask = (\n",
    "                    batch_df[\"docstring\"].notna() &\n",
    "                    (batch_df[\"docstring\"] != \"\") &\n",
    "                    (batch_df[\"docstring\"].astype(str).str.lower() != \"nan\")\n",
    "                )\n",
    "                batch_df = batch_df[valid_mask]\n",
    "                \n",
    "                if batch_df.empty:\n",
    "                    continue\n",
    "\n",
    "                function_bodies = batch_df[\"function_code\"].to_list()\n",
    "                function_docstrings = batch_df[\"docstring\"].to_list()\n",
    "                graph_batch_mask_prep = batch_df['original_index'].to_list()\n",
    "\n",
    "                graph_batch_mask = torch.zeros(function_x.size(0), dtype=torch.bool).to(model.device)\n",
    "                graph_batch_mask[graph_batch_mask_prep] = True\n",
    "\n",
    "                #print(\"Generating with logits, for loss calculation\")\n",
    "                output = model(\n",
    "                    function_bodys=function_bodies,\n",
    "                    code_graph=graph,\n",
    "                    code_graph_batch_mask=graph_batch_mask,\n",
    "                    docstrings=function_docstrings,\n",
    "                    mode=\"train\"\n",
    "                )\n",
    "\n",
    "                total_val_comb_loss += output[\"loss\"].item()\n",
    "                total_val_ce_loss += output[\"ce_loss\"].item()\n",
    "                total_val_cos_loss += output[\"cos_loss\"].item()\n",
    "                valid_val_samples += 1\n",
    "                \n",
    "        comb_loss = total_comb_loss / valid_samples if valid_samples > 0 else 0.0\n",
    "        ce_loss = total_ce_loss / valid_samples if valid_samples > 0 else 0.0\n",
    "        cos_loss = total_cos_loss / valid_samples if valid_samples > 0 else 0.0\n",
    "        val_comb_loss = total_val_comb_loss / valid_val_samples if valid_val_samples > 0 else 0.0\n",
    "        val_ce_loss = total_val_ce_loss / valid_val_samples if valid_val_samples > 0 else 0.0\n",
    "        val_cos_loss = total_val_cos_loss / valid_val_samples if valid_val_samples > 0 else 0.0\n",
    "\n",
    "        comb_losses.append(comb_loss)\n",
    "        ce_losses.append(ce_loss)\n",
    "        cos_losses.append(cos_loss)\n",
    "        val_comb_losses.append(val_comb_loss)\n",
    "        val_ce_losses.append(val_ce_loss)\n",
    "        val_cos_losses.append(val_cos_loss)\n",
    "        # Print epoch results\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"Loss: {comb_loss:.4f} | \"\n",
    "            f\"CE Loss: {ce_loss:.4f} | \"\n",
    "            f\"Cos Loss: {cos_loss:.4f}\\n\"\n",
    "            f\"Validation Loss: {val_comb_loss:.4f} | \"\n",
    "            f\"Validation CE Loss: {val_ce_loss:.4f} | \"\n",
    "            f\"Validation Cos Loss: {val_cos_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    # --- Final Generation After Training ---\n",
    "    model.eval()\n",
    "    generated_docstrings = []\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, len(val_dataset), batch_size):\n",
    "            batch_df = val_dataset.iloc[idx:idx+batch_size]\n",
    "            valid_mask = (\n",
    "                batch_df[\"docstring\"].notna() &\n",
    "                (batch_df[\"docstring\"] != \"\") &\n",
    "                (batch_df[\"docstring\"].astype(str).str.lower() != \"nan\")\n",
    "            )\n",
    "            batch_df = batch_df[valid_mask]\n",
    "            if batch_df.empty:\n",
    "                continue\n",
    "\n",
    "            function_bodies = batch_df[\"function_code\"].tolist()\n",
    "            graph_batch_mask_prep = batch_df['original_index'].tolist()\n",
    "            graph_batch_mask = torch.zeros(function_x.size(0), dtype=torch.bool).to(model.device)\n",
    "            graph_batch_mask[graph_batch_mask_prep] = True\n",
    "\n",
    "            output = model(\n",
    "                function_bodys=function_bodies,\n",
    "                code_graph=graph,\n",
    "                code_graph_batch_mask=graph_batch_mask,\n",
    "                mode=\"generate\"\n",
    "            )\n",
    "            generated_docstrings.extend(output[\"generated\"])\n",
    "\n",
    "    # --- Evaluate Generation ---\n",
    "\n",
    "    val_dataset_nonan = val_dataset[\n",
    "        (val_dataset['docstring'].notna()) &  # not NaN\n",
    "        (val_dataset['docstring'] != \"\") &    # not empty string\n",
    "        (val_dataset['docstring'].astype(str).str.lower() != \"nan\")  # not string \"NaN\"\n",
    "    ]\n",
    "    val_dataset_nonan[\"docstring_gen_multimodal\"] = generated_docstrings\n",
    "    bleu_scores, meteor_scores = zip(*[evaluate_docstring(original, generated) for original, generated in zip(val_dataset_nonan[\"docstring\"].to_list(), generated_docstrings)])\n",
    "    val_dataset_nonan[\"bleu\"] = bleu_scores\n",
    "    val_dataset_nonan[\"meteor\"] = meteor_scores\n",
    "\n",
    "    print(f\"Validation BLEU: {sum(bleu_scores) / len(bleu_scores):.4f} | \"\n",
    "            f\"Validation METEOR: {sum(meteor_scores) / len(meteor_scores):.4f}\")\n",
    "    return comb_losses, ce_losses, cos_losses, val_comb_losses, val_ce_losses, val_cos_losses, val_dataset_nonan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6332f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"graph/manim/cg_nodes.csv\")  \n",
    "graph = torch.load(\"graph/manim/hg.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f36a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(3.8848, dtype=torch.float16, grad_fn=<DivBackward0>), 'ce_loss': tensor(3.3809, dtype=torch.float16, grad_fn=<DivBackward0>), 'cos_loss': tensor(1.0088, dtype=torch.float16, grad_fn=<DivBackward0>)}\n",
      "Backward prop completed. Optimizer step done.\n",
      "{'loss': tensor(3.4609, dtype=torch.float16, grad_fn=<DivBackward0>), 'ce_loss': tensor(2.9551, dtype=torch.float16, grad_fn=<DivBackward0>), 'cos_loss': tensor(1.0127, dtype=torch.float16, grad_fn=<DivBackward0>)}\n",
      "Backward prop completed. Optimizer step done.\n",
      "Skipping empty docstring at index 2\n",
      "{'loss': tensor(4.8516, dtype=torch.float16, grad_fn=<DivBackward0>), 'ce_loss': tensor(4.3477, dtype=torch.float16, grad_fn=<DivBackward0>), 'cos_loss': tensor(1.0068, dtype=torch.float16, grad_fn=<DivBackward0>)}\n",
      "Backward prop completed. Optimizer step done.\n",
      "{'loss': tensor(5.3164, dtype=torch.float16, grad_fn=<DivBackward0>), 'ce_loss': tensor(4.8086, dtype=torch.float16, grad_fn=<DivBackward0>), 'cos_loss': tensor(1.0186, dtype=torch.float16, grad_fn=<DivBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "model = MultimodalCommentPipeline(model='Salesforce/codegen-350M-mono')\n",
    "\n",
    "comb_losses, ce_losses, cos_losses, val_comb_losses, val_ce_losses, val_cos_losses, val_dataset_nonan = train(\n",
    "        model=model,\n",
    "        dataset=dataset,\n",
    "        graph=graph,\n",
    "        num_epochs=5,\n",
    "        lr=1e-3,\n",
    "        batch_size=4\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
